{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit assignment through regression discontinuity design\n",
    "\n",
    "\n",
    "## 1. The problem\n",
    "\n",
    "In the following we consider a small network of spiking neurons $\\mathbf{h} \\in \\mathbb{R}^n$ which is able to observe a cost function/reward signal, $C(x,y,W)$, for some computation that it is involved in. \n",
    "\n",
    "To adjust its weights to minimize this cost function a neuron must know the gradient:\n",
    "$$\n",
    "\\beta_i \\equiv \\frac{\\partial C}{\\partial h_i}.\n",
    "$$\n",
    "\n",
    "This can be computed through backpropagation, $\\beta_i^{BP} \\equiv \\beta_i$, however there are few known biologically plausible mechanisms for implementing backprop. Though some candidates have appeared recently. For instance the method of synthetic gradients uses reinforcement-learning style estimators for the gradient, $\\hat{\\beta}_i^{SG} \\approx \\beta_i^{BP}$, which addresses some of the issues with backprop.\n",
    "\n",
    "Alternatively, in cases where the cost function is observed by the neuron, it could learn $\\beta_i$ from this feedback. A na\\\"ive approximation of $\\beta_i$ could proceed via a type of finite differences approach:\n",
    "$$\n",
    "\\beta_i^{FD} \\equiv \\mathbb{E}(C|h_i = 1) - \\mathbb{E}(C|h_i=0).\n",
    "$$\n",
    "Learning with such approaches is known to converge slowly (citations?). (also biased?)\n",
    "\n",
    "An issue with the above estimator is that it is effectively only measuring the correlation between $h_i$ and $C$. The adage 'correlation does not imply causation' is relevant here. Note that under the hypothetical scenario that backprop has a plausible implementation mechanism, backprop provides a _causal_ measure of $\\beta_i$. That is, via backprop the neuron learns directly about its effect on $C$ through all of its downstream connections. If, at some point downstream of $h_i$, all of the weights through which $h_i$ may affect $C$ are zero then backprop will correctly report $\\beta_i = 0$. \n",
    "\n",
    "The estimator based on $\\beta_i^{FD}$ on the other hand can be biased by correlations with other neurons. For instance, we can imagine in the same no-effect scenario ($\\beta_i = 0$) that $\\beta_i^{FD}$ could be non-zero provided there was  another neuron $h_j$ whose activity was correlated with $h_i$ and for which there were non-zero downstream weights such that $\\beta_j \\ne 0$. \n",
    "\n",
    "(This could all be written out more explicitly for a simple 2 neuron example...)\n",
    "\n",
    "The credit assignment problem in this correlated activity case is thus a challenge: how does a neuron learn _its_ specific effect on a cost function when its neighbors are responding in a similar way to an input stimulus? Spike train correlations and their effect on coding have been well-studied (Shea-Brown studies...). (How much have correlations been studied in the context of learning?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Estimation of $\\beta_i$\n",
    "\n",
    "## 2.2 Regression discontinuity design\n",
    "\n",
    "Here we propose a modification of the FD estimator that draws on methods from causal inference, and that can recover an unbiased estimator of $\\beta_i$, thus obviating the need for backprop.\n",
    "\n",
    "(Review RDD)\n",
    "\n",
    "The estimator thus takes the form\n",
    "$$\n",
    "\\beta_i^{RD} \\equiv \\lim_{x \\to \\mu^+} \\mathbb{E}(C|v_i = x) - \\lim_{x \\to \\mu^-} \\mathbb{E}(C|v_i=x)\n",
    "$$\n",
    "\n",
    "## 2.3 An RD learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model\n",
    "\n",
    "We demonstrate this idea via a simple test system of $n=2$ leaky integrate and fire (LIF) neurons. The neurons thus obey\n",
    "$$\n",
    "\\dot{v}_i = -g_L v_i + w_i\\eta_i\n",
    "$$\n",
    "for $i = 1,2$. Integrate and fire means simply:\n",
    "$$\n",
    "v_i(t^+) = v_r, \\quad \\text{when }v_i(t) = v_{th}.\n",
    "$$\n",
    "\n",
    "Noisy input $\\eta_i$ is comprised of a common DC current, $x$, and noise term, $\\xi(t)$, plus an individual noise term, $\\xi_i(t)$:\n",
    "$$\n",
    "\\eta_i(t) = x + \\sigma_i\\left[\\sqrt{1-c}\\xi_i(t) + \\sqrt{c}\\xi(t)\\right].\n",
    "$$\n",
    "The noise processes are independent white noise: $\\mathbb{E}(\\xi_i(t)\\xi_j(t')) = \\delta_{ij}\\delta(t-t')$. This parameterization is chosen so that the inputs $\\eta_{1,2}$ have correlation coefficient $c$. Output spike trains are defined as\n",
    "$$\n",
    "y_i(t) = \\sum_s \\delta(t-t_i^s)\n",
    "$$\n",
    "for $t_i^s$ the $s$th spike time.\n",
    "\n",
    "Simulations are performed with a step size of $\\Delta t = 1ms$. Thus outputs at each time step become indicator functions:\n",
    "$$\n",
    "h_i^t = \\mathbb{1}(\\text{$i$ spikes in timebin $t$}).\n",
    "$$\n",
    "\n",
    "In each timebin a cost function can then be defined as\n",
    "$$\n",
    "C_t(x, \\mathbf{h}_t, \\mathbf{w}) = (v_1 h_1^t + v_2h_2^t - x^2)^2.\n",
    "$$\n",
    "\n",
    "We simulate this system below, and show how the various estimators of $\\beta_i$ relate to each other for different values of $c$ and $v_{1,2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rand\n",
    "\n",
    "dt = 0.001\n",
    "t = 5\n",
    "gL = 0.6\n",
    "mu = 1\n",
    "reset = 0\n",
    "xsigma = 1\n",
    "n = 2\n",
    "tau = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Initialize voltage and spike train variables\n",
    "v = np.zeros((n,T))\n",
    "h = np.zeros((n,T))\n",
    "vt = np.zeros((n,1))\n",
    "T = np.ceil(t/dt)\n",
    "\n",
    "#Choose a random input x, and input and output weights\n",
    "x = rand.randn()*xsigma\n",
    "W = rand.randn(2,1)\n",
    "V = rand.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "range() integer step argument expected, got float.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8350b1d8800e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Simulate T seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: range() integer step argument expected, got float."
     ]
    }
   ],
   "source": [
    "#Simulate T seconds\n",
    "for t in range(T):\n",
    "    \n",
    "    #Sample some noise\n",
    "    \n",
    "    #\n",
    "    \n",
    "    vt = vt + dt*dv\n",
    "    \n",
    "#Compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
